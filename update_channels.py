import json
import requests
import os
import concurrent.futures
import shutil
import time
import logging
import tempfile
from datetime import datetime

# --- ‚öôÔ∏è CONFIGURATION (Ultimate) ---
BASE_DIR = os.getcwd()
CATEGORY_DIR = os.path.join(BASE_DIR, "categories")
BACKUP_DIR = os.path.join(BASE_DIR, "backups")

# ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ï‡¶Ü‡¶™ ‡¶ï‡¶®‡¶´‡¶ø‡¶ó‡¶æ‡¶∞‡ßá‡¶∂‡¶®: ‡¶™‡ßç‡¶∞‡¶§‡¶ø ‡¶´‡¶æ‡¶á‡¶≤‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶∏‡¶∞‡ßç‡¶¨‡ßã‡¶ö‡ßç‡¶ö ‡¶ï‡¶§‡¶ó‡ßÅ‡¶≤‡ßã ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ï‡¶Ü‡¶™ ‡¶∞‡¶æ‡¶ñ‡¶¨‡ßá‡¶®?
MAX_BACKUPS_TO_KEEP = 5 

# API Endpoints
STREAMS_API = "https://iptv-org.github.io/api/streams.json"
CHANNELS_API = "https://iptv-org.github.io/api/channels.json"

# Default Assets
DEFAULT_LOGO = "https://raw.githubusercontent.com/iptv-org/api/master/data/categories/no-logo.png"

# Filter Rules
CATEGORY_RULES = {
    "bangladesh.json": {"type": "country", "filter": "BD", "category_name": "Bangladesh"},
    "india.json": {"type": "country", "filter": "IN", "category_name": "India"},
    "usa.json": {"type": "country", "filter": "US", "category_name": "USA"},
    "uk.json": {"type": "country", "filter": "GB", "category_name": "UK"},
    "uae.json": {"type": "country", "filter": "AE", "category_name": "UAE"},
    "sports.json": {"type": "genre", "filter": ["sports"], "category_name": "Sports"},
    "kids.json": {"type": "genre", "filter": ["kids", "animation"], "category_name": "Kids"},
    "music.json": {"type": "genre", "filter": ["music"], "category_name": "Music"},
    "informative.json": {"type": "genre", "filter": ["documentary", "education", "science"], "category_name": "Informative"}
}

# --- üìù LOGGING SETUP ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - [%(levelname)s] - %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger()

# --- üõ°Ô∏è SAFETY & CLEANUP FUNCTIONS ---

def cleanup_old_backups():
    """‡¶™‡ßÅ‡¶∞‡¶æ‡¶®‡ßã ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ï‡¶Ü‡¶™ ‡¶´‡¶æ‡¶á‡¶≤ ‡¶Ö‡¶ü‡ßã‡¶Æ‡ßá‡¶ü‡¶ø‡¶ï ‡¶°‡¶ø‡¶≤‡¶ø‡¶ü ‡¶ï‡¶∞‡ßá (Clean Storage)‡•§"""
    if not os.path.exists(BACKUP_DIR):
        return

    logger.info("üßπ Checking for old backups to clean...")
    all_backups = [f for f in os.listdir(BACKUP_DIR) if f.endswith(".bak")]
    
    deleted_count = 0
    # ‡¶ï‡ßç‡¶Ø‡¶æ‡¶ü‡¶æ‡¶ó‡¶∞‡¶ø ‡¶Ö‡¶®‡ßÅ‡¶Ø‡¶æ‡ßü‡ßÄ ‡¶ö‡ßá‡¶ï ‡¶ï‡¶∞‡¶æ
    for filename in CATEGORY_RULES.keys():
        # ‡¶è‡¶á ‡¶®‡¶ø‡¶∞‡ßç‡¶¶‡¶ø‡¶∑‡ßç‡¶ü ‡¶´‡¶æ‡¶á‡¶≤‡ßá‡¶∞ ‡¶∏‡¶¨ ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ï‡¶Ü‡¶™ ‡¶ñ‡ßÅ‡¶Å‡¶ú‡ßá ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ
        file_backups = [f for f in all_backups if f.startswith(f"{filename}_")]
        
        # ‡¶§‡¶æ‡¶∞‡¶ø‡¶ñ ‡¶Ö‡¶®‡ßÅ‡¶Ø‡¶æ‡ßü‡ßÄ ‡¶∏‡¶æ‡¶ú‡¶æ‡¶®‡ßã (Oldest first)
        file_backups.sort()
        
        # ‡¶Ø‡¶¶‡¶ø MAX_BACKUPS_TO_KEEP ‡¶è‡¶∞ ‡¶ö‡ßá‡ßü‡ßá ‡¶¨‡ßá‡¶∂‡¶ø ‡¶•‡¶æ‡¶ï‡ßá, ‡¶§‡¶¨‡ßá ‡¶™‡ßÅ‡¶∞‡¶æ‡¶®‡ßã‡¶ó‡ßÅ‡¶≤‡ßã ‡¶°‡¶ø‡¶≤‡¶ø‡¶ü ‡¶ï‡¶∞‡ßã
        if len(file_backups) > MAX_BACKUPS_TO_KEEP:
            files_to_delete = file_backups[:-MAX_BACKUPS_TO_KEEP] # ‡¶®‡¶§‡ßÅ‡¶® ‡ß´‡¶ü‡¶ø ‡¶∞‡ßá‡¶ñ‡ßá ‡¶¨‡¶æ‡¶ï‡¶ø ‡¶∏‡¶¨ ‡¶°‡¶ø‡¶≤‡¶ø‡¶ü
            
            for old_file in files_to_delete:
                try:
                    os.remove(os.path.join(BACKUP_DIR, old_file))
                    logger.info(f"   üóëÔ∏è Auto-Deleted Old Backup: {old_file}")
                    deleted_count += 1
                except Exception as e:
                    logger.warning(f"   ‚ö†Ô∏è Failed to delete {old_file}: {e}")
    
    if deleted_count == 0:
        logger.info("   ‚úÖ No old backups needed deletion.")

def create_backup(filepath):
    """‡¶∏‡ßá‡¶≠ ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶Ü‡¶ó‡ßá ‡¶´‡¶æ‡¶á‡¶≤‡ßá‡¶∞ ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ï‡¶Ü‡¶™ ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡ßá‡•§"""
    if not os.path.exists(filepath):
        return
    
    if not os.path.exists(BACKUP_DIR):
        os.makedirs(BACKUP_DIR)
        
    filename = os.path.basename(filepath)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    backup_path = os.path.join(BACKUP_DIR, f"{filename}_{timestamp}.bak")
    
    try:
        shutil.copy2(filepath, backup_path)
        logger.info(f"üõ°Ô∏è Backup created: {backup_path}")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Backup failed: {e}")

def atomic_save_json(filepath, data):
    """‡¶°‡¶æ‡¶ü‡¶æ ‡¶∏‡ßá‡¶≠ ‡¶ï‡¶∞‡ßá ‡¶è‡¶¨‡¶Ç ‡¶ü‡ßá‡¶Æ‡ßç‡¶™‡ßã‡¶∞‡¶æ‡¶∞‡¶ø ‡¶´‡¶æ‡¶á‡¶≤ ‡¶Ö‡¶ü‡ßã‡¶Æ‡ßá‡¶ü‡¶ø‡¶ï ‡¶∞‡¶ø‡¶Æ‡ßÅ‡¶≠ ‡¶ï‡¶∞‡ßá‡•§"""
    dir_name = os.path.dirname(filepath)
    
    # ‡ßß. ‡¶ü‡ßá‡¶Æ‡ßç‡¶™‡ßã‡¶∞‡¶æ‡¶∞‡¶ø ‡¶´‡¶æ‡¶á‡¶≤ ‡¶§‡ßà‡¶∞‡¶ø
    with tempfile.NamedTemporaryFile('w', dir=dir_name, delete=False, encoding='utf-8') as tmp_file:
        json.dump(data, tmp_file, indent=2, ensure_ascii=False)
        temp_name = tmp_file.name
    
    # ‡ß®. ‡¶∏‡ßá‡¶≠ ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶ö‡ßá‡¶∑‡ßç‡¶ü‡¶æ (Replace logic)
    try:
        shutil.move(temp_name, filepath)
        # shutil.move ‡¶∏‡¶´‡¶≤ ‡¶π‡¶≤‡ßá ‡¶∏‡ßã‡¶∞‡ßç‡¶∏ (temp) ‡¶´‡¶æ‡¶á‡¶≤ ‡¶Ö‡¶ü‡ßã‡¶Æ‡ßá‡¶ü‡¶ø‡¶ï ‡¶°‡¶ø‡¶≤‡¶ø‡¶ü ‡¶π‡ßü‡ßá ‡¶Ø‡¶æ‡ßü
        logger.info(f"üíæ Safely saved & Temp file cleaned: {os.path.basename(filepath)}")
    except Exception as e:
        logger.error(f"‚ùå Save failed: {e}")
        # ‡ß©. ‡¶´‡ßá‡¶á‡¶≤ ‡¶ï‡¶∞‡¶≤‡ßá ‡¶Æ‡ßç‡¶Ø‡¶æ‡¶®‡ßÅ‡ßü‡¶æ‡¶≤‡¶ø ‡¶ü‡ßá‡¶Æ‡ßç‡¶™ ‡¶´‡¶æ‡¶á‡¶≤ ‡¶°‡¶ø‡¶≤‡¶ø‡¶ü
        if os.path.exists(temp_name):
            os.remove(temp_name)
            logger.info("   üßπ Residual Temp file removed manually.")

def load_json(filepath):
    if os.path.exists(filepath):
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                return json.load(f)
        except json.JSONDecodeError:
            logger.error(f"‚ùå Corrupted JSON found: {filepath}. Starting empty.")
            return {"channels": []}
    return {"channels": []}

# --- üåê NETWORK FUNCTIONS ---

def get_headers():
    return {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36",
        "Accept": "*/*",
        "Connection": "keep-alive"
    }

def check_link_status(url):
    """Advanced Check: ‡¶≤‡¶ø‡¶ô‡ßç‡¶ï‡¶ü‡¶ø ‡¶≤‡¶æ‡¶á‡¶≠ ‡¶ï‡¶ø‡¶®‡¶æ ‡¶ö‡ßá‡¶ï ‡¶ï‡¶∞‡ßá"""
    if not url: return False
    try:
        with requests.get(url, headers=get_headers(), stream=True, timeout=(3.05, 5), allow_redirects=True) as response:
            if response.status_code == 200:
                content_type = response.headers.get('Content-Type', '').lower()
                if 'application/x-mpegurl' in content_type or 'video' in content_type or 'octet-stream' in content_type:
                    return True
                return True 
            return False
    except:
        return False

def process_stream_check(stream, details):
    url = stream.get('url')
    ch_id = stream.get('channel')
    if check_link_status(url):
        return (ch_id, url, details)
    return None

# --- üöÄ MAIN LOGIC ---

def update_channels_pro():
    logger.info("üöÄ Starting Ultimate Channel Updater (Clean Mode)...")
    
    # ‡¶∂‡ßÅ‡¶∞‡ßÅ‡¶§‡ßá ‡¶™‡ßÅ‡¶∞‡¶®‡ßã ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ï‡¶Ü‡¶™ ‡¶°‡¶ø‡¶≤‡¶ø‡¶ü ‡¶ï‡¶∞‡¶æ
    cleanup_old_backups()

    try:
        logger.info("üì° Fetching global IPTV database...")
        api_streams = requests.get(STREAMS_API, timeout=10).json()
        api_channels = requests.get(CHANNELS_API, timeout=10).json()
        channel_info_map = {c['id']: c for c in api_channels}
        logger.info(f"‚úÖ Loaded {len(api_channels)} channels and {len(api_streams)} streams.")
    except Exception as e:
        logger.critical(f"‚ùå Critical API Error: {e}")
        return

    if not os.path.exists(CATEGORY_DIR):
        os.makedirs(CATEGORY_DIR)

    for filename, rules in CATEGORY_RULES.items():
        filepath = os.path.join(CATEGORY_DIR, filename)
        logger.info(f"\nüîç Processing Category: {rules['category_name']} ({filename})")

        current_data = load_json(filepath)
        existing_ids = {ch['id'] for ch in current_data.get('channels', [])}
        
        streams_to_check = []
        for stream in api_streams:
            ch_id = stream.get('channel')
            if not ch_id or ch_id in existing_ids: continue
            if stream.get('status') in ['error', 'offline']: continue

            ch_details = channel_info_map.get(ch_id)
            if not ch_details: continue

            is_match = False
            if rules['type'] == 'country':
                if ch_details.get('country') == rules['filter']: is_match = True
            elif rules['type'] == 'genre':
                api_cats = [c.lower() for c in ch_details.get('categories', [])]
                for target in rules['filter']:
                    if target.lower() in api_cats:
                        is_match = True
                        break
            
            if is_match:
                already_queued = any(s[0].get('channel') == ch_id for s in streams_to_check)
                if not already_queued:
                    streams_to_check.append((stream, ch_details))

        if not streams_to_check:
            logger.info("   üò¥ No new channels found.")
            continue

        logger.info(f"   ‚ö° Found {len(streams_to_check)} potential NEW channels. Checking liveness...")

        new_channels_list = []
        with concurrent.futures.ThreadPoolExecutor(max_workers=20) as executor:
            future_to_url = {
                executor.submit(process_stream_check, s, d): s 
                for s, d in streams_to_check
            }

            for future in concurrent.futures.as_completed(future_to_url):
                result = future.result()
                if result:
                    ch_id, url, details = result
                    
                    api_logo = details.get('logo')
                    final_logo = api_logo if api_logo else DEFAULT_LOGO

                    new_channel = {
                        "id": ch_id,
                        "name": details.get('name', 'Unknown Channel'),
                        "logoUrl": final_logo,
                        "streamUrls": [url],
                        "category": rules['category_name']
                    }
                    if rules['type'] == 'genre':
                         new_channel["genre"] = rules['category_name']
                    
                    new_channels_list.append(new_channel)
                    print(f"     ‚úÖ [LIVE] {details.get('name')}")

        if new_channels_list:
            new_channels_list.sort(key=lambda x: x['name'])
            
            logger.info(f"   üì• Adding {len(new_channels_list)} confirmed live channels.")
            
            # ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ï‡¶Ü‡¶™ ‡¶§‡ßà‡¶∞‡¶ø
            create_backup(filepath)
            
            # ‡¶°‡¶æ‡¶ü‡¶æ ‡¶Ü‡¶™‡¶°‡ßá‡¶ü
            current_data['channels'].extend(new_channels_list)
            
            # ‡¶∏‡ßá‡¶≠ ‡¶è‡¶¨‡¶Ç ‡¶ü‡ßá‡¶Æ‡ßç‡¶™ ‡¶´‡¶æ‡¶á‡¶≤ ‡¶ï‡ßç‡¶≤‡¶ø‡¶®‡¶Ü‡¶™
            atomic_save_json(filepath, current_data)
        else:
            logger.info("   ‚ö†Ô∏è Potential channels found, but none were live.")

    logger.info("\nüéâ All updates and cleanups completed successfully!")

if __name__ == "__main__":
    update_channels_pro()

